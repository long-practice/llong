{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터 50,000개<br>\n",
    "검증 데이터, 테스트 데이터 10,000개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = X[:50000], X[50000:60000], X[60000:], Y[:50000], Y[50000:60000], Y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_valid = X_valid / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, 1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, l = 0):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    cost = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1)) + 1 / 2 * l * np.sum(W ** 2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 cost에 규제항 추가<br>\n",
    "$\\mbox{cost} = \\mbox{cost} - \\lambda \\left\\Vert \\boldsymbol{W} \\right\\Vert_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 검증함수 구현\n",
    "def validate(X_valid, y_valid, W_optimal):\n",
    "    X_val = np.hstack((np.ones((np.size(X_valid, 0), 1)), X_valid))\n",
    "    T_val = y_valid\n",
    "    y_pred = predict(X_val, W_optimal)\n",
    "    score = float(sum(y_pred == np.argmax(T_val, axis=1))) / float(len(y_valid))\n",
    "\n",
    "    print(f\"score = {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.RandomState(seed=42).permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "    \n",
    "    acc_history = []\n",
    "    \n",
    "    lambdas = [-(10 ** -x) for x in range(10, 0, -1)] + [0] + [10 ** -x for x in range(1, 11)]\n",
    "    \n",
    "    for idx, l in enumerate(lambdas):\n",
    "        K = np.size(T, 1)\n",
    "        M = np.size(X, 1)\n",
    "        W = np.zeros((M,K))\n",
    "        for i in range(iterations):\n",
    "            j = i % N\n",
    "            X_batch = X_shuffled[j:j+batch_size]\n",
    "            T_batch = T_shuffled[j:j+batch_size]\n",
    "            # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "            if X_batch.shape[0] < batch_size:\n",
    "                X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "                T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "            W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch)) - l * W\n",
    "            cost_history[i] = compute_cost(X_batch, T_batch, W, l)\n",
    "            # 10000번 학습당 cost 출력\n",
    "#             if i % 10000 == 0:\n",
    "#                 print(cost_history[i][0])\n",
    "                \n",
    "        # 검증 데이터에 대해 결과를 검증하고\n",
    "        # 정확도를 기록 \n",
    "        print('lamda:', l)\n",
    "        acc = validate(X_valid, y_valid, W)\n",
    "        print('accuracy:', acc)\n",
    "#         print('가중치', W)\n",
    "        acc_history.append((lambdas[idx], acc))\n",
    "        print(acc_history)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 손실함수 $\\mbox{cost} = \\mbox{cost} + \\lambda \\left\\Vert \\boldsymbol{W} \\right\\Vert_2^2$에서 규제항을 미분<br>\n",
    "$\\nabla_{\\boldsymbol{W}} = \\lambda \\left\\Vert \\boldsymbol{W} \\right\\Vert_2^2 = \\nabla_{\\boldsymbol{W}} (\\lambda \\boldsymbol{W}^T \\boldsymbol{W}) = 2\\lambda \\boldsymbol{W} \\rightarrow \\lambda \\boldsymbol{W}$<br>\n",
    "이에 따라 손실함수는 다음과 같이 변경(상수항을 $\\lambda$로 계산하기 때문)<br>\n",
    "$\\displaystyle \\mbox{cost} = \\mbox{cost} + \\frac{1}{2}\\lambda \\left\\Vert \\boldsymbol{W} \\right\\Vert_2^2$<br>\n",
    "<br>\n",
    "따라서 가중치 업데이트 시 $\\lambda \\boldsymbol{W}$ 항을 추가하여 최적의 정확도를 보장하는 $\\lambda$를 구하면됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (50000, 785)\n",
      "T shape: (50000, 10)\n",
      "Initial Cost is: 2.3024850979937375 \n",
      "\n",
      "lamda: -1e-10\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -1e-09\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -1e-08\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -1e-07\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -1e-06\n",
      "score = 0.9203\n",
      "accuracy: 0.9203\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -1e-05\n",
      "score = 0.9214\n",
      "accuracy: 0.9214\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -0.0001\n",
      "score = 0.0991\n",
      "accuracy: 0.0991\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -0.001\n",
      "score = 0.0991\n",
      "accuracy: 0.0991\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -0.01\n",
      "score = 0.0991\n",
      "accuracy: 0.0991\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: -0.1\n",
      "score = 0.0991\n",
      "accuracy: 0.0991\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 0\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 0.1\n",
      "score = 0.2138\n",
      "accuracy: 0.2138\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 0.01\n",
      "score = 0.6572\n",
      "accuracy: 0.6572\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 0.001\n",
      "score = 0.8558\n",
      "accuracy: 0.8558\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572), (0.001, 0.8558)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 0.0001\n",
      "score = 0.9033\n",
      "accuracy: 0.9033\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572), (0.001, 0.8558), (0.0001, 0.9033)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 1e-05\n",
      "score = 0.918\n",
      "accuracy: 0.918\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572), (0.001, 0.8558), (0.0001, 0.9033), (1e-05, 0.918)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 1e-06\n",
      "score = 0.9198\n",
      "accuracy: 0.9198\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572), (0.001, 0.8558), (0.0001, 0.9033), (1e-05, 0.918), (1e-06, 0.9198)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 1e-07\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572), (0.001, 0.8558), (0.0001, 0.9033), (1e-05, 0.918), (1e-06, 0.9198), (1e-07, 0.9201)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 1e-08\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572), (0.001, 0.8558), (0.0001, 0.9033), (1e-05, 0.918), (1e-06, 0.9198), (1e-07, 0.9201), (1e-08, 0.9201)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 1e-09\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572), (0.001, 0.8558), (0.0001, 0.9033), (1e-05, 0.918), (1e-06, 0.9198), (1e-07, 0.9201), (1e-08, 0.9201), (1e-09, 0.9201)]\n",
      "\n",
      "\n",
      "\n",
      "lamda: 1e-10\n",
      "score = 0.9201\n",
      "accuracy: 0.9201\n",
      "[(-1e-10, 0.9201), (-1e-09, 0.9201), (-1e-08, 0.9201), (-1e-07, 0.9201), (-1e-06, 0.9203), (-1e-05, 0.9214), (-0.0001, 0.0991), (-0.001, 0.0991), (-0.01, 0.0991), (-0.1, 0.0991), (0, 0.9201), (0.1, 0.2138), (0.01, 0.6572), (0.001, 0.8558), (0.0001, 0.9033), (1e-05, 0.918), (1e-06, 0.9198), (1e-07, 0.9201), (1e-08, 0.9201), (1e-09, 0.9201), (1e-10, 0.9201)]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "print('X shape:', X.shape)\n",
    "print('T shape:', T.shape)\n",
    "\n",
    "initial_cost = compute_cost(X, T, W)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda = -0.00001$ 부근에서 높은 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.RandomState(seed=42).permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "    \n",
    "    acc_history = []\n",
    "    \n",
    "    lambdas = [- x / 10 ** 6 for x in range(5, 99)]\n",
    "    \n",
    "    for idx, l in enumerate(lambdas):\n",
    "        K = np.size(T, 1)\n",
    "        M = np.size(X, 1)\n",
    "        W = np.zeros((M,K))\n",
    "        for i in range(iterations):\n",
    "            j = i % N\n",
    "            X_batch = X_shuffled[j:j+batch_size]\n",
    "            T_batch = T_shuffled[j:j+batch_size]\n",
    "            # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "            if X_batch.shape[0] < batch_size:\n",
    "                X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "                T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "            W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch)) - l * W\n",
    "            cost_history[i] = compute_cost(X_batch, T_batch, W, l)\n",
    "            # 10000번 학습당 cost 출력\n",
    "#             if i % 10000 == 0:\n",
    "#                 print(cost_history[i][0])\n",
    "                \n",
    "        # 검증 데이터에 대해 결과를 검증하고\n",
    "        # 정확도를 기록 \n",
    "        acc = validate(X_valid, y_valid, W)\n",
    "#         print('가중치', W)\n",
    "        acc_history.append(acc)\n",
    "\n",
    "    return (cost_history, W), acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (50000, 785)\n",
      "T shape: (50000, 10)\n",
      "Initial Cost is: 2.3024850979937375 \n",
      "\n",
      "score = 0.9208\n",
      "score = 0.9206\n",
      "score = 0.921\n",
      "score = 0.9208\n",
      "score = 0.9211\n",
      "score = 0.9214\n",
      "score = 0.922\n",
      "score = 0.9223\n",
      "score = 0.9226\n",
      "score = 0.9223\n",
      "score = 0.9219\n",
      "score = 0.922\n",
      "score = 0.9223\n",
      "score = 0.9219\n",
      "score = 0.9221\n",
      "score = 0.9222\n",
      "score = 0.9224\n",
      "score = 0.9227\n",
      "score = 0.9225\n",
      "score = 0.9227\n",
      "score = 0.9225\n",
      "score = 0.9221\n",
      "score = 0.9221\n",
      "score = 0.9219\n",
      "score = 0.9218\n",
      "score = 0.9218\n",
      "score = 0.9216\n",
      "score = 0.9219\n",
      "score = 0.9218\n",
      "score = 0.9216\n",
      "score = 0.9215\n",
      "score = 0.9214\n",
      "score = 0.921\n",
      "score = 0.9208\n",
      "score = 0.9214\n",
      "score = 0.9213\n",
      "score = 0.9217\n",
      "score = 0.9214\n",
      "score = 0.9213\n",
      "score = 0.9209\n",
      "score = 0.9211\n",
      "score = 0.9209\n",
      "score = 0.9203\n",
      "score = 0.9201\n",
      "score = 0.9199\n",
      "score = 0.92\n",
      "score = 0.9201\n",
      "score = 0.9198\n",
      "score = 0.9196\n",
      "score = 0.9197\n",
      "score = 0.9194\n",
      "score = 0.919\n",
      "score = 0.9185\n",
      "score = 0.9182\n",
      "score = 0.9181\n",
      "score = 0.918\n",
      "score = 0.9177\n",
      "score = 0.9176\n",
      "score = 0.9178\n",
      "score = 0.9174\n",
      "score = 0.9177\n",
      "score = 0.9176\n",
      "score = 0.917\n",
      "score = 0.917\n",
      "score = 0.9172\n",
      "score = 0.9172\n",
      "score = 0.9169\n",
      "score = 0.9167\n",
      "score = 0.9166\n",
      "score = 0.9164\n",
      "score = 0.9164\n",
      "score = 0.9163\n",
      "score = 0.9161\n",
      "score = 0.9158\n",
      "score = 0.9157\n",
      "score = 0.9158\n",
      "score = 0.9153\n",
      "score = 0.9148\n",
      "score = 0.9149\n",
      "score = 0.915\n",
      "score = 0.9145\n",
      "score = 0.9143\n",
      "score = 0.914\n",
      "score = 0.9139\n",
      "score = 0.9138\n",
      "score = 0.9133\n",
      "score = 0.0991\n",
      "score = 0.0991\n",
      "score = 0.0991\n",
      "score = 0.0991\n",
      "score = 0.0991\n",
      "score = 0.0991\n",
      "score = 0.0991\n",
      "score = 0.0991\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "print('X shape:', X.shape)\n",
    "print('T shape:', T.shape)\n",
    "\n",
    "initial_cost = compute_cost(X, T, W)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal), acc_li = batch_gd(X, T, W, learning_rate, iterations, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.2e-05, 0.9227)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = [- x / 10 ** 6 for x in range(5, 99)]\n",
    "lambda_max = lambdas[np.argmax(acc_li)]\n",
    "acc_max = np.max(acc_li)\n",
    "lambda_max, acc_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최대 정확도 일 때에는 $\\lambda = -0.000022$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size, l):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch)) - l * W\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W, l)\n",
    "        # 1000번 학습당 cost 출력\n",
    "        if i % 5000 == 0:\n",
    "            print(cost_history[i][0])\n",
    "            \n",
    "\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (50000, 785)\n",
      "T shape: (50000, 10)\n",
      "Initial Cost is: 2.3024850979937375 \n",
      "\n",
      "2.2811737901504423\n",
      "0.2890821509838653\n",
      "0.17526655310082961\n",
      "0.16062665797344375\n",
      "0.23256748743398256\n",
      "0.13629100291513951\n",
      "0.4234400327849822\n",
      "0.2792979511099775\n",
      "0.32546071891895767\n",
      "0.10408271648352072\n",
      "score = 0.9211\n",
      "score = 0.9175\n",
      "Initial Cost is: 2.3024850979937375 \n",
      "\n",
      "2.274773453446265\n",
      "0.39507944751277546\n",
      "0.3656071111535034\n",
      "0.1702711549024994\n",
      "0.3186276078715601\n",
      "0.25194470438946714\n",
      "0.20912738704593808\n",
      "0.21200073534096853\n",
      "0.2908217090680837\n",
      "0.18797097157237674\n",
      "score = 0.9181\n",
      "score = 0.9135\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 50000\n",
    "learning_rate = 0.01\n",
    "print('X shape:', X.shape)\n",
    "print('T shape:', T.shape)\n",
    "\n",
    "\n",
    "l = -0.000022\n",
    "initial_cost = compute_cost(X, T, W, l)\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64, l)\n",
    "reg_acc_val = validate(X_valid, y_valid, W_optimal)\n",
    "reg_acc_test = validate(X_test, y_test, W_optimal)\n",
    "\n",
    "l = 0\n",
    "initial_cost = compute_cost(X, T, W, l)\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 64, l)\n",
    "acc_val = validate(X_valid, y_valid, W_optimal)\n",
    "acc_test = validate(X_test, y_test, W_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid acc with regularization: 0.9211\n",
      "test acc with regularization: 0.9175\n",
      "\n",
      "valid acc without regularization: 0.9181\n",
      "test acc without regularization: 0.9135\n"
     ]
    }
   ],
   "source": [
    "# 정확도 비교\n",
    "print('valid acc with regularization:', reg_acc_val)\n",
    "print('test acc with regularization:', reg_acc_test)\n",
    "print()\n",
    "print('valid acc without regularization:', acc_val)\n",
    "print('test acc without regularization:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "눈에 띄게 성능이 향상된건 아니지만 약간의 성능 향상이 이루어졌다고 볼 수 있음(0.9135에서 0.9175로)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
